# -*- coding: utf-8 -*-
"""Segmentation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/150g9N2T-iO7E-BLcFqKKIn82ORDzGbmA
"""

import tensorflow as tf
import matplotlib.pyplot as plt
import cv2
import os
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession
import math
import pickle
link_dataset=''   #link to the original dataset
link="C:/Users/guill/OneDrive/Bureau/Devoirs/Perso/Projets Info/Segmentation_NN/"   #link to the transformed dataset
classes=[[0,0,0], [255,255,255]]
                    
def extract_dataset():
    """
        Extract and transform the dataset from link_dataset to link
        input: no input
        output: no output
    """
    count=0 
    for filename in os.listdir(link_dataset):
        if not '(2)' in filename:
            image=cv2.imread(link_dataset+'/'+filename)
            image=cv2.resize(image, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)
            cv2.imwrite(link+'Inputs/input_'+str(count)+'.png', image)
            image=cv2.imread(link_dataset+'/'+filename.split('.')[0]+' (2).png')
            image=cv2.resize(image, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)
            new_image=[]
            for i in image:
                new_image.append([])
                for j in i:
                    if 0 in j:
                        new_image[len(new_image)-1].append([1,0])
                    else:
                        new_image[len(new_image)-1].append([0,1])
            with open(link+"Outputs_classes/output_"+str(count)+".pickle", "wb") as f:
                pickle.dump(new_image, f)
            count+=1

def dataset():
    """
        Load the dataset in memory
        input: no input
        output: Xtrain: list of 80% of the inputs of the dataset for training
                Ytrain: list of 80% of the outputs of the dataset for training
                Xtest: list of 20% of the inputs of the dataset to test
                Ytest: list of 20% of the outputs of the dataset for test
                256: two dimensions of the arrays
    """
    input=[]
    output=[]
    for filename in os.listdir(link+"Inputs/"):
        image=cv2.imread(link+"Inputs/"+filename)
        input.append(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))
    for filename in os.listdir(link+"Outputs_classes/"):
        with open(link+"Outputs_classes/"+filename, "rb") as f:
            image=pickle.load(f)
        output.append(image)
        
    X,Y=shuffle(input,output)
    
    Xtrain,Xtest,Ytrain,Ytest=train_test_split(X,Y, test_size=0.2)
    
    Xtrain=np.asarray(Xtrain)
    Ytrain=np.asarray(Ytrain)
    Xtest=np.asarray(Xtest)
    Ytest=np.asarray(Ytest)

    return Xtrain,Ytrain,Xtest,Ytest,256,256

def smooth_curve(points, factor=0.9):
    """
        Smooth the plot of the neural network results (accuracy, loss) to focus on important fluctuations
        input: points: Accuracy or Loss points from neural network history
        output: smoothed_points: Accuracy or Loss points transformed
    """
    smoothed_points = []
    for point in points:
        if smoothed_points:
            previous = smoothed_points[-1]
            smoothed_points.append(previous * factor + point * (1 - factor))
        else:
            smoothed_points.append(point)
    return smoothed_points

def model(larg, haut):
    """
        Build the neural network
        input: larg, haut: dimensions of the arrays
        output: model: a tf model
    """
    inputs = tf.keras.Input(shape=(larg, haut, 3))
    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(inputs)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling2D(pool_size= (2,2), strides=2)(x)

    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling2D(pool_size= (2,2), strides=2)(x)

    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv2D(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.MaxPooling2D(pool_size= (2,2), strides=2)(x)

    x = tf.keras.layers.UpSampling2D(size= (2,2), interpolation='nearest')(x)
    x = tf.keras.layers.Conv2DTranspose(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv2DTranspose(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)

    x = tf.keras.layers.UpSampling2D(size= (2,2), interpolation='nearest')(x)
    x = tf.keras.layers.Conv2DTranspose(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv2DTranspose(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)

    x = tf.keras.layers.UpSampling2D(size= (2,2), interpolation='nearest')(x)
    x = tf.keras.layers.Conv2DTranspose(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)
    x = tf.keras.layers.Conv2DTranspose(16, kernel_size=3, activation='relu',kernel_initializer='he_uniform', padding = 'same')(x)
    x = tf.keras.layers.Dropout(0.2)(x)
    x = tf.keras.layers.BatchNormalization()(x)

    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)


    model = tf.keras.Model(inputs=inputs, outputs=outputs, name="model")

    model.summary()

    return model

def compilate_model(model, Xtrain, Ytrain, Xtest, Ytest):
    """
        Set the parameters (callbacks, learning rate...) and run the neural network 
        input: model: a tf model
               Xtrain: list of 80% of the inputs of the dataset for training
               Ytrain: list of 80% of the outputs of the dataset for training
               Xtest: list of 20% of the inputs of the dataset to test
               Ytest: list of 20% of the outputs of the dataset for test 
        output: history: Samples of Accuracy and loss to evaluate the training
                saved_model: trained model
    """
    loss_history = LossHistory()
    lrate = tf.keras.callbacks.LearningRateScheduler(exp_decay)
    es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='auto', verbose=0, patience=20)
    mc = tf.keras.callbacks.ModelCheckpoint(link+'best_model.h5', monitor='val_accuracy', mode='max', verbose=0, save_best_only=True)
    model.compile(optimizer='adam', loss ='categorical_crossentropy', metrics = ['accuracy'])
    history = model.fit(Xtrain,Ytrain, validation_data=(Xtest,Ytest), epochs=300, batch_size=8, verbose=0, callbacks=[es, mc, lrate, loss_history])
    saved_model = tf.keras.models.load_model(link+'best_model.h5')
    return history, saved_model

def show_results(history):
    """
        Show the graphs of Accuracy and Loss with matplotlib
        input: history: Samples of Accuracy and loss to evaluate the training
        output: no output
    """
    plt.plot(smooth_curve(history.history['accuracy']), label='train')
    plt.plot(smooth_curve(history.history['val_accuracy']), label='test')
    plt.title('accuracy:', pad=-80)
    plt.show()
    plt.plot(smooth_curve(history.history['loss']), label='train')
    plt.plot(smooth_curve(history.history['val_loss']), label='test')
    plt.title('loss:', pad=-80)
    plt.show()

def predict(image, larg, haut):
    """
        Predict the segmentation of a new image not in the dataset and plot it
        input: saved_model: trained model
               image: image to predict
               larg, haut: dimensions of the image
        output: no output
    """
    saved_model = tf.keras.models.load_model(link+'best_model.h5')
    test=cv2.imread(link+"Tests/"+image)
    test=cv2.cvtColor(test,cv2.COLOR_BGR2RGB)
    test=cv2.resize(test, dsize=(larg, haut), interpolation=cv2.INTER_CUBIC)
    test = tf.expand_dims(test, 0)
    predict=saved_model.predict(test)
    predict=tf.squeeze(predict)
    new_predict=[[classes[list(j).index(float(max(j)))] for j in i] for i in predict]
    test=tf.squeeze(test)
    plt.imshow(test)
    plt.show()
    plt.imshow(new_predict)
    
def step_decay(epoch):
    """
        Decrease the learning rate following a step mathematical function
        input: epoch: State of training of the network
        output: lrate: parameter of the gradient descent
    """
    initial_lrate = 0.001
    drop = 0.00001
    epochs_drop = 15.0
    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))
    return lrate

def exp_decay(epoch):
    """
        Decrease the learning rate following a exponential mathematical function
        input: epoch: State of training of the network
        output: lrate: parameter of the gradient descent
    """
    initial_lrate = 0.001
    k = 0.00005
    lrate = initial_lrate * math.exp(-k*epoch)
    return lrate
    
class LossHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
       self.losses = []
       self.lr = []
 
    def on_epoch_end(self, batch, logs={}):
       self.losses.append(logs.get('loss'))
       self.lr.append(exp_decay(len(self.losses)))
    
if __name__=='__main__':
    tf.debugging.set_log_device_placement(True)
    config = ConfigProto()
    config.gpu_options.allow_growth = True     #comment this line if you're not running with a GPU
    session = InteractiveSession(config=config)
    """with tf.device('/device:CPU:0'):        #train the model
        Xtrain,Ytrain,Xtest,Ytest,larg,haut=dataset()
        model=model(larg,haut)
    history, model=compilate_model(model,Xtrain,Ytrain,Xtest,Ytest)
    show_results(history)"""
    predict('test1.png', 256, 256)             #make a prediction on an input unknowned by the model
    session.close()